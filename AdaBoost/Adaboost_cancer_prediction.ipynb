{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, youâ€™ll explore the breast cancer dataset and try to train the model to predict if the person is having breast cancer or not. We will start off with a weak learner, a decision tree with maximum depth = 2.\n",
    "\n",
    "We will then build an adaboost ensemble with 50 trees with a step of 3 and compare the performance with the weak learner.\n",
    "\n",
    "Let's get started by loading the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the breast cancer dataset in which the target variable has 1 if the person has cancer and 0 otherwise. Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "digits = load_digits()\n",
    "\n",
    "data = cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= np.c_[data['data'], data['target']],\n",
    "                     columns= list(data['feature_names']) + ['target'])\n",
    "df['target'] = df['target'].astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         17.990         10.38          122.80     1001.0          0.11840   \n",
       "1         20.570         17.77          132.90     1326.0          0.08474   \n",
       "2         19.690         21.25          130.00     1203.0          0.10960   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "4         20.290         14.34          135.10     1297.0          0.10030   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "6         18.250         19.98          119.60     1040.0          0.09463   \n",
       "7         13.710         20.83           90.20      577.9          0.11890   \n",
       "8         13.000         21.82           87.50      519.8          0.12730   \n",
       "9         12.460         24.04           83.97      475.9          0.11860   \n",
       "10        16.020         23.24          102.70      797.8          0.08206   \n",
       "11        15.780         17.89          103.60      781.0          0.09710   \n",
       "12        19.170         24.80          132.40     1123.0          0.09740   \n",
       "13        15.850         23.95          103.70      782.7          0.08401   \n",
       "14        13.730         22.61           93.60      578.3          0.11310   \n",
       "15        14.540         27.54           96.73      658.8          0.11390   \n",
       "16        14.680         20.13           94.74      684.5          0.09867   \n",
       "17        16.130         20.68          108.10      798.8          0.11700   \n",
       "18        19.810         22.15          130.00     1260.0          0.09831   \n",
       "19        13.540         14.36           87.46      566.3          0.09779   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "21         9.504         12.44           60.34      273.9          0.10240   \n",
       "22        15.340         14.26          102.50      704.4          0.10730   \n",
       "23        21.160         23.04          137.20     1404.0          0.09428   \n",
       "24        16.650         21.38          110.00      904.6          0.11210   \n",
       "25        17.140         16.40          116.00      912.7          0.11860   \n",
       "26        14.580         21.53           97.41      644.8          0.10540   \n",
       "27        18.610         20.25          122.10     1094.0          0.09440   \n",
       "28        15.300         25.27          102.40      732.4          0.10820   \n",
       "29        17.570         15.05          115.00      955.1          0.09847   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "539        7.691         25.44           48.34      170.4          0.08668   \n",
       "540       11.540         14.44           74.65      402.9          0.09984   \n",
       "541       14.470         24.99           95.81      656.4          0.08837   \n",
       "542       14.740         25.42           94.70      668.6          0.08275   \n",
       "543       13.210         28.06           84.88      538.4          0.08671   \n",
       "544       13.870         20.70           89.77      584.8          0.09578   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "546       10.320         16.35           65.31      324.9          0.09434   \n",
       "547       10.260         16.58           65.85      320.8          0.08877   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "549       10.820         24.21           68.89      361.6          0.08192   \n",
       "550       10.860         21.48           68.51      360.5          0.07431   \n",
       "551       11.130         22.44           71.49      378.4          0.09566   \n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "553        9.333         21.94           59.01      264.0          0.09240   \n",
       "554       12.880         28.92           82.50      514.3          0.08123   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "556       10.160         19.59           64.73      311.7          0.10030   \n",
       "557        9.423         27.88           59.26      271.3          0.08123   \n",
       "558       14.590         22.68           96.39      657.1          0.08473   \n",
       "559       11.510         23.93           74.52      403.5          0.09261   \n",
       "560       14.050         27.15           91.38      600.4          0.09929   \n",
       "561       11.200         29.37           70.67      386.0          0.07449   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "563       20.920         25.09          143.00     1347.0          0.10990   \n",
       "564       21.560         22.39          142.00     1479.0          0.11100   \n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "566       16.600         28.08          108.30      858.1          0.08455   \n",
       "567       20.600         29.33          140.10     1265.0          0.11780   \n",
       "568        7.760         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760        0.300100             0.147100         0.2419   \n",
       "1             0.07864        0.086900             0.070170         0.1812   \n",
       "2             0.15990        0.197400             0.127900         0.2069   \n",
       "3             0.28390        0.241400             0.105200         0.2597   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "5             0.17000        0.157800             0.080890         0.2087   \n",
       "6             0.10900        0.112700             0.074000         0.1794   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "8             0.19320        0.185900             0.093530         0.2350   \n",
       "9             0.23960        0.227300             0.085430         0.2030   \n",
       "10            0.06669        0.032990             0.033230         0.1528   \n",
       "11            0.12920        0.099540             0.066060         0.1842   \n",
       "12            0.24580        0.206500             0.111800         0.2397   \n",
       "13            0.10020        0.099380             0.053640         0.1847   \n",
       "14            0.22930        0.212800             0.080250         0.2069   \n",
       "15            0.15950        0.163900             0.073640         0.2303   \n",
       "16            0.07200        0.073950             0.052590         0.1586   \n",
       "17            0.20220        0.172200             0.102800         0.2164   \n",
       "18            0.10270        0.147900             0.094980         0.1582   \n",
       "19            0.08129        0.066640             0.047810         0.1885   \n",
       "20            0.12700        0.045680             0.031100         0.1967   \n",
       "21            0.06492        0.029560             0.020760         0.1815   \n",
       "22            0.21350        0.207700             0.097560         0.2521   \n",
       "23            0.10220        0.109700             0.086320         0.1769   \n",
       "24            0.14570        0.152500             0.091700         0.1995   \n",
       "25            0.22760        0.222900             0.140100         0.3040   \n",
       "26            0.18680        0.142500             0.087830         0.2252   \n",
       "27            0.10660        0.149000             0.077310         0.1697   \n",
       "28            0.16970        0.168300             0.087510         0.1926   \n",
       "29            0.11570        0.098750             0.079530         0.1739   \n",
       "..                ...             ...                  ...            ...   \n",
       "539           0.11990        0.092520             0.013640         0.2037   \n",
       "540           0.11200        0.067370             0.025940         0.1818   \n",
       "541           0.12300        0.100900             0.038900         0.1872   \n",
       "542           0.07214        0.041050             0.030270         0.1840   \n",
       "543           0.06877        0.029870             0.032750         0.1628   \n",
       "544           0.10180        0.036880             0.023690         0.1620   \n",
       "545           0.06747        0.029740             0.024430         0.1664   \n",
       "546           0.04994        0.010120             0.005495         0.1885   \n",
       "547           0.08066        0.043580             0.024380         0.1669   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "549           0.06602        0.015480             0.008160         0.1976   \n",
       "550           0.04227        0.000000             0.000000         0.1661   \n",
       "551           0.08194        0.048240             0.022570         0.2030   \n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "553           0.05605        0.039960             0.012820         0.1692   \n",
       "554           0.05824        0.061950             0.023430         0.1566   \n",
       "555           0.07658        0.059990             0.027380         0.1593   \n",
       "556           0.07504        0.005025             0.011160         0.1791   \n",
       "557           0.04971        0.000000             0.000000         0.1742   \n",
       "558           0.13300        0.102900             0.037360         0.1454   \n",
       "559           0.10210        0.111200             0.041050         0.1388   \n",
       "560           0.11260        0.044620             0.043040         0.1537   \n",
       "561           0.03558        0.000000             0.000000         0.1060   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "563           0.22360        0.317400             0.147400         0.2149   \n",
       "564           0.11590        0.243900             0.138900         0.1726   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "566           0.10230        0.092510             0.053020         0.1590   \n",
       "567           0.27700        0.351400             0.152000         0.2397   \n",
       "568           0.04362        0.000000             0.000000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "5                   0.07613  ...          23.75           103.40       741.6   \n",
       "6                   0.05742  ...          27.66           153.20      1606.0   \n",
       "7                   0.07451  ...          28.14           110.60       897.0   \n",
       "8                   0.07389  ...          30.73           106.20       739.3   \n",
       "9                   0.08243  ...          40.68            97.65       711.4   \n",
       "10                  0.05697  ...          33.88           123.80      1150.0   \n",
       "11                  0.06082  ...          27.28           136.50      1299.0   \n",
       "12                  0.07800  ...          29.94           151.70      1332.0   \n",
       "13                  0.05338  ...          27.66           112.00       876.5   \n",
       "14                  0.07682  ...          32.01           108.80       697.7   \n",
       "15                  0.07077  ...          37.13           124.10       943.2   \n",
       "16                  0.05922  ...          30.88           123.40      1138.0   \n",
       "17                  0.07356  ...          31.48           136.80      1315.0   \n",
       "18                  0.05395  ...          30.88           186.80      2398.0   \n",
       "19                  0.05766  ...          19.26            99.70       711.2   \n",
       "20                  0.06811  ...          20.49            96.09       630.5   \n",
       "21                  0.06905  ...          15.66            65.13       314.9   \n",
       "22                  0.07032  ...          19.08           125.10       980.9   \n",
       "23                  0.05278  ...          35.59           188.00      2615.0   \n",
       "24                  0.06330  ...          31.56           177.00      2215.0   \n",
       "25                  0.07413  ...          21.40           152.40      1461.0   \n",
       "26                  0.06924  ...          33.21           122.40       896.9   \n",
       "27                  0.05699  ...          27.26           139.90      1403.0   \n",
       "28                  0.06540  ...          36.71           149.30      1269.0   \n",
       "29                  0.06149  ...          19.52           134.90      1227.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "539                 0.07751  ...          31.89            54.49       223.6   \n",
       "540                 0.06782  ...          19.68            78.78       457.8   \n",
       "541                 0.06341  ...          31.73           113.50       808.9   \n",
       "542                 0.05680  ...          32.29           107.40       826.4   \n",
       "543                 0.05781  ...          37.17            92.48       629.6   \n",
       "544                 0.06688  ...          24.75            99.17       688.6   \n",
       "545                 0.05801  ...          29.09            97.58       729.8   \n",
       "546                 0.06201  ...          21.77            71.12       384.9   \n",
       "547                 0.06714  ...          22.04            71.08       357.4   \n",
       "548                 0.06235  ...          25.59            69.10       364.2   \n",
       "549                 0.06328  ...          31.45            83.90       505.6   \n",
       "550                 0.05948  ...          24.77            74.08       412.3   \n",
       "551                 0.06552  ...          28.26            77.80       436.6   \n",
       "552                 0.05637  ...          36.00            88.10       594.7   \n",
       "553                 0.06576  ...          25.05            62.86       295.8   \n",
       "554                 0.05708  ...          35.74            88.84       595.7   \n",
       "555                 0.06127  ...          34.91            69.57       357.6   \n",
       "556                 0.06331  ...          22.88            67.88       347.3   \n",
       "557                 0.06059  ...          34.24            66.50       330.6   \n",
       "558                 0.06147  ...          27.27           105.90       733.5   \n",
       "559                 0.06570  ...          37.16            82.28       474.2   \n",
       "560                 0.06171  ...          33.17           100.20       706.7   \n",
       "561                 0.05502  ...          38.30            75.19       439.6   \n",
       "562                 0.07152  ...          42.79           128.70       915.0   \n",
       "563                 0.06879  ...          29.41           179.10      1819.0   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560          0.71190   \n",
       "1             0.12380            0.18660          0.24160   \n",
       "2             0.14440            0.42450          0.45040   \n",
       "3             0.20980            0.86630          0.68690   \n",
       "4             0.13740            0.20500          0.40000   \n",
       "5             0.17910            0.52490          0.53550   \n",
       "6             0.14420            0.25760          0.37840   \n",
       "7             0.16540            0.36820          0.26780   \n",
       "8             0.17030            0.54010          0.53900   \n",
       "9             0.18530            1.05800          1.10500   \n",
       "10            0.11810            0.15510          0.14590   \n",
       "11            0.13960            0.56090          0.39650   \n",
       "12            0.10370            0.39030          0.36390   \n",
       "13            0.11310            0.19240          0.23220   \n",
       "14            0.16510            0.77250          0.69430   \n",
       "15            0.16780            0.65770          0.70260   \n",
       "16            0.14640            0.18710          0.29140   \n",
       "17            0.17890            0.42330          0.47840   \n",
       "18            0.15120            0.31500          0.53720   \n",
       "19            0.14400            0.17730          0.23900   \n",
       "20            0.13120            0.27760          0.18900   \n",
       "21            0.13240            0.11480          0.08867   \n",
       "22            0.13900            0.59540          0.63050   \n",
       "23            0.14010            0.26000          0.31550   \n",
       "24            0.18050            0.35780          0.46950   \n",
       "25            0.15450            0.39490          0.38530   \n",
       "26            0.15250            0.66430          0.55390   \n",
       "27            0.13380            0.21170          0.34460   \n",
       "28            0.16410            0.61100          0.63350   \n",
       "29            0.12550            0.28120          0.24890   \n",
       "..                ...                ...              ...   \n",
       "539           0.15960            0.30640          0.33930   \n",
       "540           0.13450            0.21180          0.17970   \n",
       "541           0.13400            0.42020          0.40400   \n",
       "542           0.10600            0.13760          0.16110   \n",
       "543           0.10720            0.13810          0.10620   \n",
       "544           0.12640            0.20370          0.13770   \n",
       "545           0.12160            0.15170          0.10490   \n",
       "546           0.12850            0.08842          0.04384   \n",
       "547           0.14610            0.22460          0.17830   \n",
       "548           0.11990            0.09546          0.09350   \n",
       "549           0.12040            0.16330          0.06194   \n",
       "550           0.10010            0.07348          0.00000   \n",
       "551           0.10870            0.17820          0.15640   \n",
       "552           0.12340            0.10640          0.08653   \n",
       "553           0.11030            0.08298          0.07993   \n",
       "554           0.12270            0.16200          0.24390   \n",
       "555           0.13840            0.17100          0.20000   \n",
       "556           0.12650            0.12000          0.01005   \n",
       "557           0.10730            0.07158          0.00000   \n",
       "558           0.10260            0.31710          0.36620   \n",
       "559           0.12980            0.25170          0.36300   \n",
       "560           0.12410            0.22640          0.13260   \n",
       "561           0.09267            0.05494          0.00000   \n",
       "562           0.14170            0.79170          1.17000   \n",
       "563           0.14070            0.41860          0.65990   \n",
       "564           0.14100            0.21130          0.41070   \n",
       "565           0.11660            0.19220          0.32150   \n",
       "566           0.11390            0.30940          0.34030   \n",
       "567           0.16500            0.86810          0.93870   \n",
       "568           0.08996            0.06444          0.00000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                 0.26540          0.4601                  0.11890       0  \n",
       "1                 0.18600          0.2750                  0.08902       0  \n",
       "2                 0.24300          0.3613                  0.08758       0  \n",
       "3                 0.25750          0.6638                  0.17300       0  \n",
       "4                 0.16250          0.2364                  0.07678       0  \n",
       "5                 0.17410          0.3985                  0.12440       0  \n",
       "6                 0.19320          0.3063                  0.08368       0  \n",
       "7                 0.15560          0.3196                  0.11510       0  \n",
       "8                 0.20600          0.4378                  0.10720       0  \n",
       "9                 0.22100          0.4366                  0.20750       0  \n",
       "10                0.09975          0.2948                  0.08452       0  \n",
       "11                0.18100          0.3792                  0.10480       0  \n",
       "12                0.17670          0.3176                  0.10230       0  \n",
       "13                0.11190          0.2809                  0.06287       0  \n",
       "14                0.22080          0.3596                  0.14310       0  \n",
       "15                0.17120          0.4218                  0.13410       0  \n",
       "16                0.16090          0.3029                  0.08216       0  \n",
       "17                0.20730          0.3706                  0.11420       0  \n",
       "18                0.23880          0.2768                  0.07615       0  \n",
       "19                0.12880          0.2977                  0.07259       1  \n",
       "20                0.07283          0.3184                  0.08183       1  \n",
       "21                0.06227          0.2450                  0.07773       1  \n",
       "22                0.23930          0.4667                  0.09946       0  \n",
       "23                0.20090          0.2822                  0.07526       0  \n",
       "24                0.20950          0.3613                  0.09564       0  \n",
       "25                0.25500          0.4066                  0.10590       0  \n",
       "26                0.27010          0.4264                  0.12750       0  \n",
       "27                0.14900          0.2341                  0.07421       0  \n",
       "28                0.20240          0.4027                  0.09876       0  \n",
       "29                0.14560          0.2756                  0.07919       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "539               0.05000          0.2790                  0.10660       1  \n",
       "540               0.06918          0.2329                  0.08134       1  \n",
       "541               0.12050          0.3187                  0.10230       1  \n",
       "542               0.10950          0.2722                  0.06956       1  \n",
       "543               0.07958          0.2473                  0.06443       1  \n",
       "544               0.06845          0.2249                  0.08492       1  \n",
       "545               0.07174          0.2642                  0.06953       1  \n",
       "546               0.02381          0.2681                  0.07399       1  \n",
       "547               0.08333          0.2691                  0.09479       1  \n",
       "548               0.03846          0.2552                  0.07920       1  \n",
       "549               0.03264          0.3059                  0.07626       1  \n",
       "550               0.00000          0.2458                  0.06592       1  \n",
       "551               0.06413          0.3169                  0.08032       1  \n",
       "552               0.06498          0.2407                  0.06484       1  \n",
       "553               0.02564          0.2435                  0.07393       1  \n",
       "554               0.06493          0.2372                  0.07242       1  \n",
       "555               0.09127          0.2226                  0.08283       1  \n",
       "556               0.02232          0.2262                  0.06742       1  \n",
       "557               0.00000          0.2475                  0.06969       1  \n",
       "558               0.11050          0.2258                  0.08004       1  \n",
       "559               0.09653          0.2112                  0.08732       1  \n",
       "560               0.10480          0.2250                  0.08321       1  \n",
       "561               0.00000          0.1566                  0.05905       1  \n",
       "562               0.23560          0.4089                  0.14090       0  \n",
       "563               0.25420          0.2929                  0.09873       0  \n",
       "564               0.22160          0.2060                  0.07115       0  \n",
       "565               0.16280          0.2572                  0.06637       0  \n",
       "566               0.14180          0.2218                  0.07820       0  \n",
       "567               0.26500          0.4087                  0.12400       0  \n",
       "568               0.00000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(455, 1)\n",
      "(114, 30)\n",
      "(114, 1)\n"
     ]
    }
   ],
   "source": [
    "# adaboost experiments\n",
    "# create x and y train\n",
    "X = df.drop('target', axis=1)\n",
    "y = df[['target']]\n",
    "\n",
    "# split data into train and test/validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target    0.626374\n",
      "dtype: float64\n",
      "target    0.631579\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the average cancer occurence rates in train and test data, should be comparable\n",
    "print(y_train.mean())\n",
    "print(y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base estimator: a weak learner with max_depth=2\n",
    "shallow_tree = DecisionTreeClassifier(max_depth=2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385964912280702"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the shallow decision tree \n",
    "shallow_tree.fit(X_train, y_train)\n",
    "\n",
    "# test error\n",
    "y_pred = shallow_tree.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see the accuracy using the AdaBoost algorithm. In this following code, we will write code to calculate the accuracy of the AdaBoost models as we increase the number of trees from 1 to 50 with a step of 3 in the lines:\n",
    "\n",
    "'estimators = list(range(1, 50, 3))'\n",
    "\n",
    "'for n_est in estimators:'\n",
    "\n",
    "We finally end up with the accuracy of all the models in a single list abc_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost with the tree as base estimator\n",
    "\n",
    "estimators = list(range(1, 50, 3))\n",
    "\n",
    "abc_scores = []\n",
    "for n_est in estimators:\n",
    "    ABC = AdaBoostClassifier(\n",
    "    base_estimator=shallow_tree, \n",
    "    n_estimators = n_est)\n",
    "    \n",
    "    ABC.fit(X_train, y_train)\n",
    "    y_pred = ABC.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    abc_scores.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9473684210526315,\n",
       " 0.9298245614035088,\n",
       " 0.9473684210526315,\n",
       " 0.9736842105263158,\n",
       " 0.9385964912280702,\n",
       " 0.9473684210526315,\n",
       " 0.9473684210526315,\n",
       " 0.9649122807017544,\n",
       " 0.956140350877193,\n",
       " 0.956140350877193,\n",
       " 0.9736842105263158,\n",
       " 0.9824561403508771,\n",
       " 0.9649122807017544,\n",
       " 0.9736842105263158,\n",
       " 0.9824561403508771,\n",
       " 0.9824561403508771,\n",
       " 0.9824561403508771]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot test scores and n_estimators\n",
    "# plot\n",
    "plt.plot(estimators, abc_scores)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim([0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdaBoostClassifier in module sklearn.ensemble._weight_boosting:\n",
      "\n",
      "class AdaBoostClassifier(sklearn.base.ClassifierMixin, BaseWeightBoosting)\n",
      " |  AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      " |  \n",
      " |  An AdaBoost classifier.\n",
      " |  \n",
      " |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      " |  classifier on the original dataset and then fits additional copies of the\n",
      " |  classifier on the same dataset but where the weights of incorrectly\n",
      " |  classified instances are adjusted such that subsequent classifiers focus\n",
      " |  more on difficult cases.\n",
      " |  \n",
      " |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <adaboost>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  base_estimator : object, optional (default=None)\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      Support for sample weighting is required, as well as proper\n",
      " |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      " |      the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\n",
      " |  \n",
      " |  n_estimators : int, optional (default=50)\n",
      " |      The maximum number of estimators at which boosting is terminated.\n",
      " |      In case of perfect fit, the learning procedure is stopped early.\n",
      " |  \n",
      " |  learning_rate : float, optional (default=1.)\n",
      " |      Learning rate shrinks the contribution of each classifier by\n",
      " |      ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |  algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
      " |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      " |      ``base_estimator`` must support calculation of class probabilities.\n",
      " |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      " |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      " |      achieving a lower test error with fewer boosting iterations.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  estimator_weights_ : array of floats\n",
      " |      Weights for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  estimator_errors_ : array of floats\n",
      " |      Classification error for each estimator in the boosted\n",
      " |      ensemble.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The feature importances if supported by the ``base_estimator``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  AdaBoostRegressor\n",
      " |      An AdaBoost regressor that begins by fitting a regressor on the\n",
      " |      original dataset and then fits additional copies of the regressor\n",
      " |      on the same dataset but where the weights of instances are\n",
      " |      adjusted according to the error of the current prediction.\n",
      " |  \n",
      " |  GradientBoostingClassifier\n",
      " |      GB builds an additive model in a forward stage-wise fashion. Regression\n",
      " |      trees are fit on the negative gradient of the binomial or multinomial\n",
      " |      deviance loss function. Binary classification is a special case where\n",
      " |      only a single regression tree is induced.\n",
      " |  \n",
      " |  sklearn.tree.DecisionTreeClassifier\n",
      " |      A non-parametric supervised learning method used for classification.\n",
      " |      Creates a model that predicts the value of a target variable by\n",
      " |      learning simple decision rules inferred from the data features.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      " |         on-Line Learning and an Application to Boosting\", 1995.\n",
      " |  \n",
      " |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.feature_importances_\n",
      " |  array([0.28..., 0.42..., 0.14..., 0.16...])\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.983...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdaBoostClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseWeightBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : array, shape = [n_samples, k]\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a boosted classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, the sample weights are initialized to\n",
      " |          ``1 / n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict classes for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class log-probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each boosting iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each boosting iteration.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of array, shape = [n_samples, k]\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Return staged predictions for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble prediction after each\n",
      " |      iteration of boosting and therefore allows monitoring, such as to\n",
      " |      determine the prediction on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of array, shape = [n_samples]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble predicted class probabilities\n",
      " |      after each iteration of boosting and therefore allows monitoring, such\n",
      " |      as to determine the predicted class probabilities on a test set after\n",
      " |      each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      -------\n",
      " |      p : generator of array, shape = [n_samples]\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  staged_score(self, X, y, sample_weight=None)\n",
      " |      Return staged scores for X, y.\n",
      " |      \n",
      " |      This generator method yields the ensemble score after each iteration of\n",
      " |      boosting and therefore allows monitoring, such as to determine the\n",
      " |      score on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      z : float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The feature importances.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
